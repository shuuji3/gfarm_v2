非同期要求の場合、要求を発生する側は、コスト負担なしに大量の要求を出すことが
でき、要求を処理する側に過大な負荷をかけ、問題を発生させることになる。

例:
https://sourceforge.net/apps/trac/gfarm/ticket/76
https://sourceforge.net/apps/trac/gfarm/ticket/79

このため、非同期要求に関しては流量制御が必須となる。

以下のようなポリシーで制御する。
・in-flight な RPC の総数は、流量管理する。
・流量管理は、RPC の種別ごとに行なう。
　特定の RPC が資源を占有するのを防ぐため。

なお、以下では
	gfsd に対して非同期RPCを発行後、返答の返ってないもの
を、in-flight な RPC と表現する。


■ キュー一般

	・abstract_host 毎に、要素種別の数だけキューを作る。
	  (abstract_host::sendq->workqs[type])
	  要素がこの種別キューから外れるのは、送信に対応する応答受信時、
	  ないしエラー発生時。

	・種別キュー in-flight な RPC 数を管理し、その上限値 (window_size) を
	　定め制限する。

	・コネクション切断・再開後の再送があるため、送信しても返答を
	　受信するまでは種別キューから　外さない。このため、
	　種別キューの先頭 (GFARM_HCIRCLEQ_FIRST(workq->q, workq_entries))
	　と次に送信する先頭 (workq->next) という2つの先頭ポインタを持つ。

	・window_size 的に送信可能になったら、abstract_host 毎に存在する
	  送信可能キュー (abstract_host::sendq->readyq) に移す。

	※ 要素の種類別にキューを作るか？１つにするか？
		要素の種類別にキューを作る。
		キューの種別は、優先度順に並べる。
		優先度の高い要素に妨げられて、優先度の低い要素が
		送信されない状況が心配だが、
		種別キュー (abstract_host::sendq->workqs[type]) とは別に、
		種別のない送信可能キュー (abstract_host::sendq->readyq) があり、
		種別のウィンドウが開いている限り、送信可能キューには
		時間順序通り入るので、おそらく問題ない。	   

■ キュー要素の種類別の話

　 プロトコル名と、window size のリミット設定の対応
	プロトコル名			設定
	GFM_PROTO_* reply to gfmd	gfm_proto_reply_to_gfmd_window
	GFM_PROTO_JOURNAL_READY_TO_RECV	不要
	GFM_PROTO_JOURNAL_SEND		gfm_proto_journal_send_window
	GFM_PROTO_PEER_ALLOC		gfm_proto_peer_alloc_window
	GFM_PROTO_PEER_FREE		gfm_proto_peer_free_window
	GFM_PROTO_RPC			gfm_proto_rpc_window
	GFM_PROTO_GFS_RPC		gfm_proto_gfs_rpc_window

	GFM_PROTO_* reply to gfsd	gfm_proto_reply_to_gfsd_window
	GFS_PROTO_STATUS		不要
	GFS_PROTO_FHSTAT		gfs_proto_fhstat_request_window
	GFS_PROTO_FHREMOVE		gfs_proto_fhremove_request_window
	GFS_PROTO_REPLICATION_REQUEST	gfs_proto_replication_request_window
		旧名: simultaneous_replication_receivers

○ GFM_PROTO_JOURNAL_READY_TO_RECV

	接続が切れた場合:
		キューの内容は捨てる

○ GFM_PROTO_JOURNAL_SEND

	接続が切れた場合:
		キューの内容は捨てる

○ GFS_PROTO_STATUS

	接続が切れた場合:
		キューの内容は捨てる

○ GFS_PROTO_FHSTAT

	接続が切れた場合:
		キューの内容は捨てる(?)

○ GFS_PROTO_FHREMOVE

	接続が切れた場合:
		キューの内容は保持

	レプリカ数が足りない場合には、dead_file_copy は存在しても
	実際に、それを gfsd へは送らない。
	これは、送信用キューとは別の単一のキュー kept に繋いで管理する。

	・ホストが up/down した場合には、キューのスキャンが必要
		up の場合:
			生きているレプリカ数が十分になったため、
			kept にあるもののうち、up したのとは別のホスト宛の
			要素の一部が、送信用キューへ移る。
			このため、kept キューを全スキャンする。
		down の場合:
			生きているレプリカ数が不足するため、
			downしたのとは別のgfsd向けキューにあるものの一部が、
			消すべきではなくなる…
			この場合、全 dead_file_copy をスキャンする必要がでる。
			XXX 今回の実装では、このケースを無視している。

○ GFS_PROTO_REPLICATION_REQUEST

	世代更新が終るまでは、複製要求を出せない。
	→ https://sourceforge.net/apps/trac/gfarm/ticket/184
	このため、それまでは struct inode_activity に保持しておき、
	世代更新完了後、送信用キューに移す。

	接続が切れた場合:
		ユーザーからの{src+dst ホスト指定,dstのみ指定}での要求
			dst 側が切れたわけで、諦める。
		既存複製の更新のための要求
			→複製数に関する制約を満たすためのスケジュール
			　XXX 未実装
		複製数に関する制約を満たすための要求
			→制約条件に従い、再度スケジュール
			　XXX 未実装

■ 書換え前のスレッド構造

○ back channel 関連

- FHSTAT 要求
  使ってない

- FHREMOVE 要求
  送信: remover() スレッドが単一のキューから抜き出して、
        送信スレッドプールに対し、thrpool_add_job。
  受信: 受信スレッドプール下のスレッドが、removal_finishedq_enqueue で
	キューに加える。
	独立した removal_finalizer() スレッドが、このキューから取り出して
	処理する。

- STATUS 要求
  送信: (最初の1回目を除き) callout() で送信スレッドプールを使い、定期的に
	スケジュール。
  受信: 受信スレッドプール下のスレッドが、直接処理する。

- REPLICATION_REQUEST 要求
  送信: クライアントに対する foreground 処理の中で、送信スレッドプール
	に対し thrpool_add_job
  受信: 受信スレッドプール下のスレッドが、直接処理する。

- GFM_PROTO_REPLICATION_RESULT 返答
  送信: 受信スレッドプール下のスレッドが、直接処理する。

○ gfmd channel 関連

書き換え前

- gfmdc_send_thread_pool
	作ってるが、使ってない。

- gfmdc_journal_asyncsend_thread()
	async metadata replication は、すべてここで行なっている。
	gfmd.c:start_gfmdc_threads() で生成。
	全ホストに対して gfmdc_journal_asyncsend_each_mdhost() を
	呼び出してから 0.5秒スリープする…という処理を無限ループする。

- gfmdc_journal_asyncsend_each_mdhost()
	引数に渡されたmdhostが自分でも同期メタデータレプリケーションでも
	なければ、gfmdc_journal_asyncsend() を呼び出す。

- gfmdc_journal_asyncsend() 

■ 書換え後のスレッド構造

3種類のキューを設ける。

・abstract_host 毎に、NETSENDQ_TYPE_GF?_PROTO_NUM_TYPES 個ずつ存在する
  abstract_host::sendq->workqs[type]
	送信可能待ちになってから、受信完了されるまで、要求を保持するキュー。
	FHREMOVE の場合、ホストが down しても、リトライを行なうため、
	このキューに保持し続ける。

・abstract_host 毎に、1つずつ存在する abstract_host::sendq->readyq
	送信スレッドの起動を担当するスレッドである netsendq_manager に
	渡す要求のキュー

・back_channel 用に全体で1つ、gfmd_channel 用に全体で一つ、合計2つだけ存在する
  netsenq_manager::hostq
	netsendq_manager が監視しているのは、これ。
	abstract_host::sendq->readyq にエントリが存在しているホストのリスト。

※ GFS_PROTO_FHREMOVE のみ、netsendq_type::flags に
　 NETSENDQ_FLAG_QUEUEABLE_IF_DOWN を設定しておく。
　 なお FHREMOVE であっても、一度 abstract_host::sendq->readyq に移った後は、
　 ホストが down したら abstract_host::sendq->workqs[type] から外れる。
　 外れた要素は、FHREMOVE 用の finalize 関数で、再び種別キューに追加される。
　 このようにしている理由は、workq->next の管理を単純化するためと、
　 種別キューに再追加する条件を細かく制御するため。
　 （フレームワーク側に、細かい条件を書いてしまうと、融通が効かない）

※ GFS_PROTO_FHREMOVE のみ、netsendq_type::flags に
　 NETSENDQ_FLAG_PRIOR_ONE_SHOT を設定しておく。

送信側:

(1) 元々の送信要求側スレッドは、以下を行なう。

(1.1) abstract_host 毎にある、
   要求の種類別キュー abstract_host::sendq->workqs[type] に要求を追加する。
   ただし、NETSENDQ_FLAG_QUEUEABLE_IF_DOWN 以外は、その abstract_host が
   down していたら、キューに入れずにエラーを返す。

(1.2) (1.1) で要求の種類別キュー abstract_host::sendq->workqs[type] に
  追加した結果、ウィンドウ制御的な意味で送信可能だった場合は、
  さらに abstract_host 毎にある、送信可能キュー abstract_host::sendq->readyq に
  も追加する。
  このとき、NETSENDQ_FLAG_PRIOR_ONE_SHOT であれば、送信可能キューの先頭に
  加える。

(1.3) 元々の要求側のスレッドは、この abstract_host が、netsendq_manager 
　の監視する、送信待ちホスト・キュー netsendq_manager::hostq に登録されている
  かどうか調べ、登録されていなければ、netsendq_manager::hostq に追加する。
  これで、元々の要求側のスレッドは送信処理を完了する。
  これにより、以下を保証している。
	元々の要求側とは別のスレッド (== netsendq_send_manager()) が、
	thrpool_add_job を行なうため、送信用スレッドプールが満杯でも、
	元々の要求側スレッドが待たされることがない。

(2) netsendq_send_manager() は独立したスレッドであり、送信待ちホスト・キュー
　以下の点について、netsendq_manager::hostq を監視する。
	・要素が追加される
	・ある宛先について送信が完了し、次の送信が可能になる。
  これにより、送信可能になったホストを検知したら、以下を行なう。
  その送信先へ、現在送信中でないことを確認し、
  abstract_host::sendq->readyq から、要求を抜き出す。
  次に、その送信先が up していることを確認し、
  up していれば thrpool_add_job() を行なう。
  その送信先が down している場合は、netsendq_host_is_down_at_entry() を行なう。

(3) 送信が完了したら、その送信先が、
  送信待ちホスト・キュー netsendq_manager::hostq
  に繋がれているか否か調べ、繋がれていたら、netsendq_manager を起床させる。
	→ netsendq_was_sent_to_host()

(4) 受信処理が完了したら、受信の結果ウィンドウが開いたかどうか確認し、
  開いたのであれば、abstract_host::sendq->workqs[type] の次のエントリーに、
  対し、上記(2)以降の処理を行なう。

(5) そのホストが down した場合、
  abstract_host::sendq->readyq のエントリーは、すべて削除する。
  abstract_host::sendq->workqs[type] で、まだ送信に至ってないエントリーは、
  netsendq_host_is_down_at_entry() を行なう。

受信側:

(1) giant lock のような、大きなロックが不要であれば、受信スレッドプール下
  のスレッドで直接処理する。

(2) giant lock が必要な処理、すなわち FHREMOVE や REPLICATION_RESULT などの
  処理については、netsendq_finalizer 用のキューに追加する。

(3) netsendq_finalizer は独立したスレッドであり、netsendq_finalizer 専用の
  キューに要素が加えられたら、自スレッドで片付け処理を行なう。
  これにより、以下を保証している。
	受信用スレッドプールを、giant lock 等で封鎖しない。
	これにより、デッドロック等を避ける。
	XXX 送信処理や受信処理とことなり、netsendq_finalizer はシングル
	    スレッドの実装となっている。マルチスレッド化すべきか？

■ 外部インターフェース

モジュール初期化	netsendq_manager_new
キュー初期化		netsendq_newq
キュー・エントリ初期化	netsendq_entry_init
キューに追加		netsendq_add_entry
送信完了通知		netsendq_was_sent_to_host	※ エラーでも呼ぶこと
送信後・受信完了通知	netsendq_remove_entry		※ エラーでも呼ぶこと
ホスト削除通知		netsendq_host_remove
ホストdown通知		netsendq_host_becomes_down
ホストup通知		netsendq_host_becomes_up
